{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal for this notebook is to tackle the Classic Control problems found on OpenAI's Gym. These problems include **Acrobot**, **CartPole**, **MountainCar**, and **Pendulum**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v0\n",
    "\n",
    "The CartPole is a very simple and classic example of a reinforcement learning problem first described by Sutton, Barto, and Anderson [Barto83]. From the OpenAI Gym website, \"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Problem\n",
    "\n",
    "I want to get a better understanding of the CartPole problem. What are the possible actions, what are the states, etc. I'll start off with some exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Action space: Discrete(2)\n",
      "Observation space: Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so this gives some insight, it looks like there are two possible actions to take, 0 or 1, and the state consists of 4 values, let's look into the bounds of those values to get a feel for our state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space upper bound: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Observation space lower bound: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space upper bound:\", env.observation_space.high)\n",
    "print(\"Observation space lower bound:\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so I immediately see some issues here, two of the observation values can take on a very large range of values 6.8e+38. This can definitely cause some issues when creating our value functions, there are a large number of possible state configurations. For now we are going to ignore it and see if it really does cause issues in practice. We're going to start off with using Sarsa for an on-policy TD approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def cartpole_sarsa(env, epsilon = 0.1, num_of_iter=10000, discount = 0.9):\n",
    "    Q = defaultdict(lambda: np.array([0,0])) # These action values are arbitrarily 0 and represent the actions 0 and 1.\n",
    "    \n",
    "    for iter_i in range(1, num_of_iter):\n",
    "        current_state = tuple(env.reset()[1::2])\n",
    "        done = False\n",
    "        if((iter_i-1) % 1000 == 0):\n",
    "            print(\"Finished iteration {}\".format(iter_i+1))\n",
    "        while not done:\n",
    "            policy = e_soft_policy_from_Q_for_state(Q, current_state, epsilon)\n",
    "            action = np.random.choice(Q[current_state], p=policy)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = tuple(next_state[1::2])\n",
    "            \n",
    "            Q[current_state] = (Q[current_state] + (1 / iter_i) * \n",
    "                                (reward + discount * max(Q[next_state]) - Q[current_state]))\n",
    "            current_state = next_state\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_soft_policy_from_Q_for_state(Q, state, epsilon):\n",
    "    state_actions = Q[state]\n",
    "    \n",
    "    max_action = np.argmax(state_actions)\n",
    "    probabilities = np.zeros(len(state_actions)) # np.zeros_like did not work for some reason\n",
    "    \n",
    "    for i in range(len(state_actions)):\n",
    "        if i == max_action:\n",
    "            probabilities[i] = 1 - epsilon + epsilon / len(state_actions)\n",
    "        else:\n",
    "            probabilities[i] = epsilon / len(state_actions)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, iterations=5):\n",
    "    for _ in range(iterations):\n",
    "        state = tuple(env.reset())\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            print(policy[state])\n",
    "            state, reward, done, info = env.step(np.random.choice([0, 1], p=policy[state]))\n",
    "            state = tuple(state)\n",
    "                                                 \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Finished iteration 2\n",
      "Finished iteration 1002\n",
      "Finished iteration 2002\n",
      "Finished iteration 3002\n",
      "Finished iteration 4002\n",
      "Finished iteration 5002\n",
      "Finished iteration 6002\n",
      "Finished iteration 7002\n",
      "Finished iteration 8002\n",
      "Finished iteration 9002\n",
      "Finished iteration 10002\n",
      "Finished iteration 11002\n",
      "Finished iteration 12002\n",
      "Finished iteration 13002\n",
      "Finished iteration 14002\n",
      "Finished iteration 15002\n",
      "Finished iteration 16002\n",
      "Finished iteration 17002\n",
      "Finished iteration 18002\n",
      "Finished iteration 19002\n",
      "Finished iteration 20002\n",
      "Finished iteration 21002\n",
      "Finished iteration 22002\n",
      "Finished iteration 23002\n",
      "Finished iteration 24002\n",
      "Finished iteration 25002\n",
      "Finished iteration 26002\n",
      "Finished iteration 27002\n",
      "Finished iteration 28002\n",
      "Finished iteration 29002\n",
      "Finished iteration 30002\n",
      "Finished iteration 31002\n",
      "Finished iteration 32002\n",
      "Finished iteration 33002\n",
      "Finished iteration 34002\n",
      "Finished iteration 35002\n",
      "Finished iteration 36002\n",
      "Finished iteration 37002\n",
      "Finished iteration 38002\n",
      "Finished iteration 39002\n",
      "Finished iteration 40002\n",
      "Finished iteration 41002\n",
      "Finished iteration 42002\n",
      "Finished iteration 43002\n",
      "Finished iteration 44002\n",
      "Finished iteration 45002\n",
      "Finished iteration 46002\n",
      "Finished iteration 47002\n",
      "Finished iteration 48002\n",
      "Finished iteration 49002\n",
      "Finished iteration 50002\n",
      "Finished iteration 51002\n",
      "Finished iteration 52002\n",
      "Finished iteration 53002\n",
      "Finished iteration 54002\n",
      "Finished iteration 55002\n",
      "Finished iteration 56002\n",
      "Finished iteration 57002\n",
      "Finished iteration 58002\n",
      "Finished iteration 59002\n",
      "Finished iteration 60002\n",
      "Finished iteration 61002\n",
      "Finished iteration 62002\n",
      "Finished iteration 63002\n",
      "Finished iteration 64002\n",
      "Finished iteration 65002\n",
      "Finished iteration 66002\n",
      "Finished iteration 67002\n",
      "Finished iteration 68002\n",
      "Finished iteration 69002\n",
      "Finished iteration 70002\n",
      "Finished iteration 71002\n",
      "Finished iteration 72002\n",
      "Finished iteration 73002\n",
      "Finished iteration 74002\n",
      "Finished iteration 75002\n",
      "Finished iteration 76002\n",
      "Finished iteration 77002\n",
      "Finished iteration 78002\n",
      "Finished iteration 79002\n",
      "Finished iteration 80002\n",
      "Finished iteration 81002\n",
      "Finished iteration 82002\n",
      "Finished iteration 83002\n",
      "Finished iteration 84002\n",
      "Finished iteration 85002\n",
      "Finished iteration 86002\n",
      "Finished iteration 87002\n",
      "Finished iteration 88002\n",
      "Finished iteration 89002\n",
      "Finished iteration 90002\n",
      "Finished iteration 91002\n",
      "Finished iteration 92002\n",
      "Finished iteration 93002\n",
      "Finished iteration 94002\n",
      "Finished iteration 95002\n",
      "Finished iteration 96002\n",
      "Finished iteration 97002\n",
      "Finished iteration 98002\n",
      "Finished iteration 99002\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "epsilon = 0.1\n",
    "number_of_iterations = 10000\n",
    "discount = 0.9\n",
    "\n",
    "Q = cartpole_sarsa(env, epsilon, number_of_iterations, discount)\n",
    "policy = defaultdict(lambda: np.array([0.5, 0.5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, av in Q.items():\n",
    "    policy[state] = e_soft_policy_from_Q_for_state(Q, state, epsilon)\n",
    "\n",
    "for k,v in policy.items():\n",
    "    if v[1] != 0.05:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n",
      "[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "test_policy(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI Gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
