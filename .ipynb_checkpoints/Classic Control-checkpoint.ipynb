{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal for this notebook is to tackle the Classic Control problems found on OpenAI's Gym. These problems include **Acrobot**, **CartPole**, **MountainCar**, and **Pendulum**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v0\n",
    "\n",
    "The CartPole is a very simple and classic example of a reinforcement learning problem first described by Sutton, Barto, and Anderson [Barto83]. From the OpenAI Gym website, \"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Problem\n",
    "\n",
    "I want to get a better understanding of the CartPole problem. What are the possible actions, what are the states, etc. I'll start off with some exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Action space: Discrete(2)\n",
      "Observation space: Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so this gives some insight, it looks like there are two possible actions to take, 0 or 1, and the state consists of 4 values, let's look into the bounds of those values to get a feel for our state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space upper bound: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Observation space lower bound: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space upper bound:\", env.observation_space.high)\n",
    "print(\"Observation space lower bound:\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so I immediately see some issues here, two of the observation values can take on a very large range of values 6.8e+38. This can definitely cause some issues when creating our value functions, there are a large number of possible state configurations. For now we are going to ignore it and see if it really does cause issues in practice. We're going to start off with using Sarsa for an on-policy TD approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is used for getting a policy back for a given state. This method takes in the current action value function and using the current state, assigns probability of taking each action in the action space for that state. In this case because the policy is epsilon soft, there is a $1 - \\epsilon + \\dfrac{\\epsilon}{|A(s)|}$ of selecting the greedy action, where $1 - \\epsilon$ reresents the probability of selecting the greedy action and $\\dfrac{\\epsilon}{|A(s)|}$ represents the probability of selecting an action completely at random. All actions than the one greedy action will have probability $\\dfrac{\\epsilon}{|A(s)|}$ of being selected. Ties between greedy actions are broken at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_soft_policy_from_Q_for_state(Q, state, epsilon):\n",
    "    state_actions = Q[tuple(state)]\n",
    "    \n",
    "    max_action = np.argmax(state_actions)\n",
    "    probabilities = np.zeros(len(state_actions)) # np.zeros_like did not work for some reason\n",
    "    \n",
    "    for i in range(len(state_actions)):\n",
    "        if i == max_action:\n",
    "            probabilities[i] = 1 - epsilon + epsilon / len(state_actions)\n",
    "        else:\n",
    "            probabilities[i] = epsilon / len(state_actions)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods here are for modularity purposes, for the sarsa algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_setup(env, Q, epsilon):\n",
    "    state = env.reset()\n",
    "    policy = e_soft_policy_from_Q_for_state(Q, state, epsilon)\n",
    "    action = np.random.choice(Q[tuple(state)], p=policy)\n",
    "    done = False\n",
    "    return state, policy, action, done\n",
    "    \n",
    "def interact_with_env(env, Q, action, policy):\n",
    "    state, reward, done, info = env.step(action)\n",
    "    next_action = np.random.choice(Q[tuple(state)], p=policy)\n",
    "    return state, next_action, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the sarsa algorithm. I initialize Q for all states with an numpy array, where each action has the value 0. This is done arbitrarily. Then I interact for a given number of episodes, getting the initial state, action, and policy. (The policy is just an e-soft policy with respect to Q) then, as long as we have not reached a terminal state, we will continually interact with the environment and update our current action_value function based on the sarsa update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def cartpole_sarsa(env, epsilon = 0.1, num_of_iter=10000, discount = 0.9, info_after_iter=5000):\n",
    "    Q = defaultdict(lambda: np.array([0,0])) # These action values are arbitrarily 0 and represent the actions 0 and 1.\n",
    "    \n",
    "    for iter_i in range(1, num_of_iter):\n",
    "        if (iter_i+1) % info_after_iter == 0:\n",
    "            print(\"Finished iteration\", iter_i+1)\n",
    "            \n",
    "        curr_state , policy, curr_action, done = get_initial_setup(env, Q, epsilon)\n",
    "        \n",
    "        while not done:\n",
    "            policy = e_soft_policy_from_Q_for_state(Q, curr_state, epsilon)\n",
    "            next_state, next_action, reward, done = interact_with_env(env, Q, curr_action, policy)\n",
    "            \n",
    "            curr_av = Q[tuple(curr_state)][curr_action]\n",
    "            next_av = Q[tuple(next_state)][next_action]\n",
    "            Q[tuple(curr_state)][curr_action] = (curr_av + (1 / iter_i) * (reward + discount * next_av - curr_av))\n",
    "            \n",
    "            curr_state = next_state\n",
    "            curr_action = next_action\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just done to test our policy out, it is another method so that we can render it out and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, iterations=5):\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            state, reward, done, info = env.step(np.random.choice([0, 1], p=policy[tuple(state)]))\n",
    "            state = state\n",
    "                                                 \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we try out our algorithm and train it on 20000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Finished iteration 5000\n",
      "Finished iteration 10000\n",
      "Finished iteration 15000\n",
      "Finished iteration 20000\n",
      "Finished iteration 25000\n",
      "Finished iteration 30000\n",
      "Finished iteration 35000\n",
      "Finished iteration 40000\n",
      "Finished iteration 45000\n",
      "Finished iteration 50000\n",
      "Finished iteration 55000\n",
      "Finished iteration 60000\n",
      "Finished iteration 65000\n",
      "Finished iteration 70000\n",
      "Finished iteration 75000\n",
      "Finished iteration 80000\n",
      "Finished iteration 85000\n",
      "Finished iteration 90000\n",
      "Finished iteration 95000\n",
      "Finished iteration 100000\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "epsilon = 0.1\n",
    "number_of_iterations = 100000\n",
    "discount = 0.9\n",
    "\n",
    "Q = cartpole_sarsa(env, epsilon, number_of_iterations, discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are testing it out and watching the performance. We initialize the policy to be random selection for all states, then update it for all the states we have visited already so that it can use that data when it encounters a new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = defaultdict(lambda: np.array([0.5, 0.5]))\n",
    "for state, av in Q.items():\n",
    "    policy[state] = e_soft_policy_from_Q_for_state(Q, state, epsilon)\n",
    "\n",
    "test_policy(env, policy, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so after running the test, it did not seem like the algorithm performed very good at all. In fact it seemed to do no better than a total random policy. Let's try and get a better understanding of what is really going on instead of just looking at how it performs visually. Let's see all the values of the probabilities and see if there is any pattern to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1035464 states we encountered.\n",
      "\n",
      "The average probability of taking action 0 across all states is 0.9498822267118915\n",
      "the average probabilitiy of taking action 1 across all states is 0.050117773288110345\n",
      "\n",
      "The median probability of taking action 0 across all states is 0.9500000000000001\n",
      "the median probabilitiy of taking action 1 across all states is 0.05\n",
      "\n",
      "The lowest probability of taking action 0 across all states is 0.5\n",
      "The lowest probability of taking action 1 across all states is 0.05\n",
      "\n",
      "The highest probability of taking action 0 across all states is 0.9500000000000001\n",
      "The highest probability of taking action 1 across all states is 0.5\n",
      "\n",
      "The number of times action 0 is greater than action 1 is 1035193\n",
      "The number of times action 1 is greater than action 0 is 0\n"
     ]
    }
   ],
   "source": [
    "states = []\n",
    "probs = []\n",
    "for state, prob in policy.items():\n",
    "    states += [state]\n",
    "    probs += [prob]\n",
    "    \n",
    "print(\"There are {} states we encountered.\".format(len(states)))\n",
    "print()\n",
    "print(\"The average probability of taking action 0 across all states is\", np.mean([prob[0] for prob in probs]))\n",
    "print(\"the average probabilitiy of taking action 1 across all states is\", np.mean([prob[1] for prob in probs]))\n",
    "print()\n",
    "print(\"The median probability of taking action 0 across all states is\", np.median([prob[0] for prob in probs]))\n",
    "print(\"the median probabilitiy of taking action 1 across all states is\", np.median([prob[1] for prob in probs]))\n",
    "print()\n",
    "print(\"The lowest probability of taking action 0 across all states is\", min([prob[0] for prob in probs]))\n",
    "print(\"The lowest probability of taking action 1 across all states is\", min([prob[1] for prob in probs]))\n",
    "print()\n",
    "print(\"The highest probability of taking action 0 across all states is\", max([prob[0] for prob in probs]))\n",
    "print(\"The highest probability of taking action 1 across all states is\", max([prob[1] for prob in probs]))\n",
    "print()\n",
    "print(\"The number of times action 0 is greater than action 1 is\", len([prob[0] for prob in probs if prob[0] > prob[1]]))\n",
    "print(\"The number of times action 1 is greater than action 0 is\", len([prob[1] for prob in probs if prob[1] > prob[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What went wrong\n",
    "\n",
    "Essentially all of the greedy actions are action 0. Why is this? Obviously always moving left is not ideal. Well, the issue stems from the fact that I have continuous data. For example, say 0.00101 and 0.0010100001 are unique, even though in this instance they should be considered equivalent compound this with the fact that there are four values that make up a state means we will **very** rarely encounter a state twice. Action 0 is greater because when a action is selected as the argmax between two of the same actions, it just chooses the first occurence of it, in this case action 0. We need to fix our representations of the state by turning out continuous data into discrete ones, by creating bins for each continuous value to fall into to. Then which 4 bins, for the 4 different values, are selected represent the state. This will allow each bin to be visited more often, and fix the oversight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2 : Turning the continous data discrete\n",
    "\n",
    "If we look back to our initial investigation into the observation space that represents the state we are in we can see that it consists of 4 values, with a range of [9.6, 6.8<sup>38</sup>, 0.82, 6.8<sup>38</sup>]. 6.8<sup>38</sup> seems incredibly high however. Is that full number space being utilized or can we create bins around a smaller area that is actually being used? Let's look more into the actual values for index 1 and 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average value of index 1 across all states is -0.9184778559366696\n",
      "The average value of index 3 across all states is 1.4126997856324721\n",
      "\n",
      "The median value of index 1 across all states is -0.9436402340398433\n",
      "the median value of index 3 across all states is 1.4260236520992366\n",
      "\n",
      "The lowest value of index 1 across all states is -2.197486857834958\n",
      "The lowest value of index 3 across all states is -2.1350244507892837\n",
      "\n",
      "The highest value of index 1 across all states is 1.5033523500609511\n",
      "The highest value of index 3 across all states is 3.3354233196470635\n",
      "\n",
      "The range of values for index 1 across all states is 3.7008392078959087\n",
      "The range of values for index 3 across all states is 5.470447770436348\n"
     ]
    }
   ],
   "source": [
    "print(\"The average value of index 1 across all states is\", np.mean([state[1] for state in states]))\n",
    "print(\"The average value of index 3 across all states is\", np.mean([state[3] for state in states]))\n",
    "print()\n",
    "print(\"The median value of index 1 across all states is\", np.median([state[1] for state in states]))\n",
    "print(\"the median value of index 3 across all states is\", np.median([state[3] for state in states]))\n",
    "print()\n",
    "min_1 = min([state[1] for state in states])\n",
    "min_3 = min([state[3] for state in states])\n",
    "print(\"The lowest value of index 1 across all states is\", min_1)\n",
    "print(\"The lowest value of index 3 across all states is\", min_3)\n",
    "print()\n",
    "max_1 = max([state[1] for state in states])\n",
    "max_3 = max([state[3] for state in states])\n",
    "print(\"The highest value of index 1 across all states is\", max_1)\n",
    "print(\"The highest value of index 3 across all states is\", max_3)\n",
    "print()\n",
    "print(\"The range of values for index 1 across all states is\", max_1 - min_1)\n",
    "print(\"The range of values for index 3 across all states is\", max_3 - min_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great**, so we can see that we're not utilizing all 6.8<sup>38</sup> values but really only a range of about 5 for each. This makes it **much** more managable and will significantly reduce our state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average value of index 0 across all states is -0.05173473956987565\n",
      "The average value of index 2 across all states is 0.07645240795956637\n",
      "\n",
      "The median value of index 0 across all states is -0.04197438123779697\n",
      "the median value of index 2 across all states is 0.05442268007662548\n",
      "\n",
      "The lowest value of index 0 across all states is -0.2745126386603193\n",
      "The lowest value of index 2 across all states is -0.20511531642439298\n",
      "\n",
      "The highest value of index 0 across all states is 0.470506246641189\n",
      "The highest value of index 2 across all states is 0.2691642443866252\n",
      "\n",
      "The range of values for index 0 across all states is 0.7450188853015083\n",
      "The range of values for index 2 across all states is 0.4742795608110182\n"
     ]
    }
   ],
   "source": [
    "print(\"The average value of index 0 across all states is\", np.mean([state[0] for state in states]))\n",
    "print(\"The average value of index 2 across all states is\", np.mean([state[2] for state in states]))\n",
    "print()\n",
    "print(\"The median value of index 0 across all states is\", np.median([state[0] for state in states]))\n",
    "print(\"the median value of index 2 across all states is\", np.median([state[2] for state in states]))\n",
    "print()\n",
    "min_0 = min([state[0] for state in states])\n",
    "min_2 = min([state[2] for state in states])\n",
    "print(\"The lowest value of index 0 across all states is\", min_0)\n",
    "print(\"The lowest value of index 2 across all states is\", min_2)\n",
    "print()\n",
    "max_0 = max([state[0] for state in states])\n",
    "max_2 = max([state[2] for state in states])\n",
    "print(\"The highest value of index 0 across all states is\", max_0)\n",
    "print(\"The highest value of index 2 across all states is\", max_2)\n",
    "print()\n",
    "print(\"The range of values for index 0 across all states is\", max_0 - min_0)\n",
    "print(\"The range of values for index 2 across all states is\", max_2 - min_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range here is even smaller for these two values, which gives us some freedom in creating the buckets. We can make them sufficiently small to reduce the amount of data lost, but large enough to make the state space navigable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_discrete_state(min_val, max_val, num_of_bins=20):\n",
    "    state = np.ones_like(([[0,0]]*num_of_bins), dtype=np.float32)\n",
    "    update_amount = (max_val - min_val) / num_of_bins\n",
    "    \n",
    "    for i in range(num_of_bins):\n",
    "        if i == 0:\n",
    "            state[i][0] = np.finfo(dtype=np.float32).min\n",
    "            state[i][1] = min_val + update_amount\n",
    "        elif i == (num_of_bins-1):\n",
    "            state[i][0] = state[i-1][1]\n",
    "            state[i][1] = np.finfo(dtype=np.float32).max\n",
    "        else:\n",
    "            state[i][0] = state[i-1][1]\n",
    "            state[i][1] = state[i-1][1] + update_amount\n",
    "            \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_soft_policy_from_Q_for_state(Q, state, epsilon):\n",
    "    state_actions = Q[tuple(state)]\n",
    "    \n",
    "    max_action = np.argmax(state_actions)\n",
    "    probabilities = np.zeros(len(state_actions)) # np.zeros_like did not work for some reason\n",
    "    \n",
    "    for i in range(len(state_actions)):\n",
    "        if i == max_action:\n",
    "            probabilities[i] = 1 - epsilon + epsilon / len(state_actions)\n",
    "        else:\n",
    "            probabilities[i] = epsilon / len(state_actions)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_setup(env, Q, epsilon):\n",
    "    state = env.reset()\n",
    "    policy = e_soft_policy_from_Q_for_state(Q, state, epsilon)\n",
    "    action = np.random.choice(Q[tuple(state)], p=policy)\n",
    "    done = False\n",
    "    return state, policy, action, done\n",
    "    \n",
    "def interact_with_env(env, Q, action, policy):\n",
    "    state, reward, done, info = env.step(action)\n",
    "    next_action = np.random.choice(Q[tuple(state)], p=policy)\n",
    "    return state, next_action, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def cartpole_sarsa(env, epsilon = 0.1, num_of_iter=10000, discount = 0.9, info_after_iter=5000):\n",
    "    Q = defaultdict(lambda: np.array([0,0])) # These action values are arbitrarily 0 and represent the actions 0 and 1.\n",
    "    \n",
    "    for iter_i in range(1, num_of_iter):\n",
    "        if (iter_i+1) % info_after_iter == 0:\n",
    "            print(\"Finished iteration\", iter_i+1)\n",
    "            \n",
    "        curr_state , policy, curr_action, done = get_initial_setup(env, Q, epsilon)\n",
    "        \n",
    "        while not done:\n",
    "            policy = e_soft_policy_from_Q_for_state(Q, curr_state, epsilon)\n",
    "            next_state, next_action, reward, done = interact_with_env(env, Q, curr_action, policy)\n",
    "            \n",
    "            curr_av = Q[tuple(curr_state)][curr_action]\n",
    "            next_av = Q[tuple(next_state)][next_action]\n",
    "            Q[tuple(curr_state)][curr_action] = (curr_av + (1 / iter_i) * (reward + discount * next_av - curr_av))\n",
    "            \n",
    "            curr_state = next_state\n",
    "            curr_action = next_action\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, iterations=5):\n",
    "    for _ in range(iterations):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            state, reward, done, info = env.step(np.random.choice([0, 1], p=policy[tuple(state)]))\n",
    "            state = state\n",
    "                                                 \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "epsilon = 0.1\n",
    "number_of_iterations = 20000\n",
    "discount = 0.9\n",
    "\n",
    "Q = cartpole_sarsa(env, epsilon, number_of_iterations, discount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI Gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
